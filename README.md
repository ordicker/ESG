# ESG project 
  We present Extremum Seeking Gradient (ESG), an innovative gradient descent method designed specifically for analog neural hardware. This method takes inspiration from control theory, where it is commonly used to optimize dynamic and unknown objective functions. In this work, we extend and adapt these control principles to the realm of training analog neural networks, where traditional backpropagation (BP) cannot be employed.
Authors:
- David Prichen
- Or Dicker

# Report
- [X] Abstract 
- [X] Introduction 
- [X] Methods 
- [X] Results (from the experiments) 
- [X] Conclusion


# Experiments 
`experiments/`
- [X] Polynomial fit
- [X] spiral fit (RNN)
- [X] MNIST 
- [-] small LLM ? (MeZO)  

# Presentation 
`presentation`
- [ ] about 5 slides 


# Code
Julia implementing under `code/julia`
